{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation Using U-NET\n",
    "\n",
    "\n",
    "Tested with: \n",
    "python = 3.9.12\n",
    "\n",
    "tensorflow = 2.9.1\n",
    "\n",
    "pillow = 9.0.1\n",
    "\n",
    "source code reference is as follows: https://github.com/CatchZeng/tensorflow-unet-labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "define the parameters for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3 # class number + 1 (background)\n",
    "INPUT_SHAPE = [224, 224, 3] # (Height, Width, Channel)\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 100\n",
    "VAL_SUBSPLITS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "define the necessary functions and class for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "def cvtColor(image):\n",
    "    \"\"\"\n",
    "    to check the RGB-image\n",
    "    \"\"\"\n",
    "    if len(np.shape(image)) == 3 and np.shape(image)[-1] == 3: \n",
    "        return image\n",
    "    else:\n",
    "        image = image.convert('RGB')\n",
    "        return image\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    \"\"\"\n",
    "    to make normalization\n",
    "    \"\"\"\n",
    "    image = image / 127.5 - 1\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize_image(image, size):\n",
    "    \"\"\"\n",
    "    to resize the images\n",
    "    \"\"\"\n",
    "    iw, ih = image.size\n",
    "    w, h = size\n",
    "\n",
    "    scale = min(w/iw, h/ih)\n",
    "    nw = int(iw*scale)\n",
    "    nh = int(ih*scale)\n",
    "\n",
    "    image = image.resize((nw, nh), Image.BICUBIC)\n",
    "    new_image = Image.new('RGB', size, (128, 128, 128)) # add gray placeholders for the place where the iamge is not proportional\n",
    "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "    return new_image, nw, nh\n",
    "\n",
    "\n",
    "def resize_label(image, size):\n",
    "    \"\"\"\n",
    "    to resize the labels\n",
    "    \"\"\"\n",
    "    iw, ih = image.size\n",
    "    w, h = size\n",
    "\n",
    "    scale = min(w/iw, h/ih)\n",
    "    nw = int(iw*scale)\n",
    "    nh = int(ih*scale)\n",
    "\n",
    "    image = image.resize((nw, nh), Image.NEAREST)\n",
    "    new_image = Image.new('L', size, (0)) # 8-bit pixels, black\n",
    "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "\n",
    "    return new_image, nw, nh\n",
    "\n",
    "\n",
    "class UnetDataset(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, annotation_lines, input_shape, batch_size, num_classes,\n",
    "                 train, dataset_path):\n",
    "        self.annotation_lines = annotation_lines\n",
    "        self.length = len(self.annotation_lines)\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.train = train\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.annotation_lines) / float(self.batch_size)) # number of batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        images = []\n",
    "        targets = []\n",
    "        for i in range(index*self.batch_size, (index+1)*self.batch_size): # get the samples in a batch\n",
    "            i = i % self.length\n",
    "            name = self.annotation_lines[i].split()[0]\n",
    "            jpg = Image.open(\n",
    "                os.path.join(os.path.join(self.dataset_path, \"JPEGImages\"),\n",
    "                             name + \".jpg\"))\n",
    "            png = Image.open(\n",
    "                os.path.join(\n",
    "                    os.path.join(self.dataset_path, \"SegmentationClassPNG\"),\n",
    "                    name + \".png\"))\n",
    "\n",
    "            jpg, png = self.process_data(jpg,\n",
    "                                         png,\n",
    "                                         self.input_shape,\n",
    "                                         random=self.train)\n",
    "\n",
    "            images.append(jpg)\n",
    "            targets.append(png)\n",
    "\n",
    "        images = np.array(images)\n",
    "        targets = np.array(targets)\n",
    "        return images, targets\n",
    "\n",
    "    def rand(self, a=0, b=1):\n",
    "        return np.random.rand() * (b - a) + a\n",
    "\n",
    "    def process_data(self, image, label, input_shape, random=True):\n",
    "        image = cvtColor(image)\n",
    "        label = Image.fromarray(np.array(label))\n",
    "        h, w, _ = input_shape\n",
    "\n",
    "        # resize\n",
    "        image, _, _ = resize_image(image, (w, h))\n",
    "        label, _, _ = resize_label(label, (w, h))\n",
    "\n",
    "        if random:\n",
    "            # flip for image augmentation\n",
    "            flip = self.rand() < .5\n",
    "            if flip:\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        # images as np array\n",
    "        image = np.array(image, np.float32)\n",
    "        image = normalize(image)\n",
    "\n",
    "        label = np.array(label)\n",
    "        label[label >= self.num_classes] = self.num_classes\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.getcwd()\n",
    "\n",
    "# read dataset txt files\n",
    "with open(os.path.join(dataset_path, \"ImageSets/Segmentation/train.txt\"),\n",
    "          \"r\",\n",
    "          encoding=\"utf8\") as f:\n",
    "    train_lines = f.readlines()\n",
    "\n",
    "with open(os.path.join(dataset_path, \"ImageSets/Segmentation/val.txt\"),\n",
    "          \"r\",\n",
    "          encoding=\"utf8\") as f:\n",
    "    val_lines = f.readlines()\n",
    "\n",
    "train_batches = UnetDataset(train_lines, INPUT_SHAPE, BATCH_SIZE, NUM_CLASSES,\n",
    "                            True, dataset_path)\n",
    "val_batches = UnetDataset(val_lines, INPUT_SHAPE, BATCH_SIZE, NUM_CLASSES,\n",
    "                          False, dataset_path)\n",
    "\n",
    "STEPS_PER_EPOCH = len(train_lines) // BATCH_SIZE\n",
    "VALIDATION_STEPS = len(val_lines) // BATCH_SIZE // VAL_SUBSPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def display(display_list):\n",
    "    \"\"\"\n",
    "    display the image and mask\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# show an image example\n",
    "images, masks = train_batches.__getitem__(0)\n",
    "sample_image, sample_mask = images[0], masks[0]\n",
    "sample_mask = sample_mask[..., tf.newaxis]\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/generative/pix2pix\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters,\n",
    "                                        size,\n",
    "                                        strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/images/segmentation\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=INPUT_SHAPE,\n",
    "                                               include_top=False)\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',  # 64x64\n",
    "    'block_3_expand_relu',  # 32x32\n",
    "    'block_6_expand_relu',  # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',  # 4x4\n",
    "]\n",
    "base_model_outputs = [\n",
    "    base_model.get_layer(name).output for name in layer_names\n",
    "]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input,\n",
    "                            outputs=base_model_outputs)\n",
    "\n",
    "down_stack.trainable = False\n",
    "\n",
    "up_stack = [\n",
    "    upsample(512, 3),  # 4x4 -> 8x8\n",
    "    upsample(256, 3),  # 8x8 -> 16x16\n",
    "    upsample(128, 3),  # 16x16 -> 32x32\n",
    "    upsample(64, 3),  # 32x32 -> 64x64\n",
    "]\n",
    "\n",
    "\n",
    "def unet_model(output_channels: int):\n",
    "    inputs = tf.keras.layers.Input(shape=INPUT_SHAPE)\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(inputs)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    # This is the last layer of the model\n",
    "    last = tf.keras.layers.Conv2DTranspose(filters=output_channels,\n",
    "                                           kernel_size=3,\n",
    "                                           strides=2,\n",
    "                                           padding='same')  #64x64 -> 128x128\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model(output_channels=NUM_CLASSES)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]\n",
    "\n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display([image[0], mask[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        display([\n",
    "            sample_image, sample_mask,\n",
    "            create_mask(model.predict(sample_image[tf.newaxis, ...]))\n",
    "        ])\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print('\\nSample Prediction after epoch {}\\n'.format(epoch + 1))\n",
    "\n",
    "\n",
    "class ModelCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self,\n",
    "                 filepath,\n",
    "                 monitor='val_loss',\n",
    "                 verbose=0,\n",
    "                 save_best_only=False,\n",
    "                 save_weights_only=False,\n",
    "                 mode='auto',\n",
    "                 period=1):\n",
    "        super(ModelCheckpointCallback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.filepath = filepath\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.period = period\n",
    "        self.epochs_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn(\n",
    "                'ModelCheckpoint mode %s is unknown, '\n",
    "                'fallback to auto mode.' % (mode), RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            if self.save_best_only:\n",
    "                current = logs.get(self.monitor)\n",
    "                if current is None:\n",
    "                    warnings.warn(\n",
    "                        'Can save best model only with %s available, '\n",
    "                        'skipping.' % (self.monitor), RuntimeWarning)\n",
    "                else:\n",
    "                    if self.monitor_op(current, self.best):\n",
    "                        if self.verbose > 0:\n",
    "                            print(\n",
    "                                '\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                                ' saving model to %s' %\n",
    "                                (epoch + 1, self.monitor, self.best, current,\n",
    "                                 filepath))\n",
    "                        self.best = current\n",
    "                        if self.save_weights_only:\n",
    "                            self.model.save_weights(filepath, overwrite=True)\n",
    "                        else:\n",
    "                            self.model.save(filepath, overwrite=True)\n",
    "                    else:\n",
    "                        if self.verbose > 0:\n",
    "                            print('\\nEpoch %05d: %s did not improve' %\n",
    "                                  (epoch + 1, self.monitor))\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('\\nEpoch %05d: saving model to %s' %\n",
    "                          (epoch + 1, filepath))\n",
    "                if self.save_weights_only:\n",
    "                    self.model.save_weights(filepath, overwrite=True)\n",
    "                else:\n",
    "                    self.model.save(filepath, overwrite=True)\n",
    "\n",
    "EarlyStop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayCallback = DisplayCallback()\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "checkpointCallback = ModelCheckpointCallback(\n",
    "    'logs/ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "    monitor='val_loss',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    period=1)\n",
    "\n",
    "model_history = model.fit(train_batches,\n",
    "                          epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=val_batches,\n",
    "                          callbacks=[displayCallback, checkpointCallback, EarlyStop])\n",
    "\n",
    "model.save('logs/the-last-model.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
    "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'logs/the-last-model.h5'\n",
    "model.load_weights(model_path)\n",
    "print('{} model loaded.'.format(model_path))\n",
    "\n",
    "import copy\n",
    "\n",
    "colors = [(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128),\n",
    "          (128, 0, 128), (0, 128, 128), (128, 128, 128),\n",
    "          (64, 0, 0), (192, 0, 0), (64, 128, 0), (192, 128, 0), (64, 0, 128),\n",
    "          (192, 0, 128), (64, 128, 128), (192, 128, 128), (0, 64, 0),\n",
    "          (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128),\n",
    "          (128, 64, 12)]\n",
    "\n",
    "\n",
    "def detect_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = cvtColor(image)\n",
    "\n",
    "    old_img = copy.deepcopy(image)\n",
    "    ori_h = np.array(image).shape[0]\n",
    "    ori_w = np.array(image).shape[1]\n",
    "\n",
    "    image_data, nw, nh = resize_image(image, (INPUT_SHAPE[1], INPUT_SHAPE[0]))\n",
    "\n",
    "    image_data = normalize(np.array(image_data, np.float32))\n",
    "\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "\n",
    "    pr = model.predict(image_data)[0]\n",
    "\n",
    "    pr = pr[int((INPUT_SHAPE[0] - nh) // 2) : int((INPUT_SHAPE[0] - nh) // 2 + nh), \\\n",
    "            int((INPUT_SHAPE[1] - nw) // 2) : int((INPUT_SHAPE[1] - nw) // 2 + nw)]\n",
    "\n",
    "    pr = cv2.resize(pr, (ori_w, ori_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    pr = pr.argmax(axis=-1)\n",
    "\n",
    "\n",
    "    seg_img = np.reshape(\n",
    "        np.array(colors, np.uint8)[np.reshape(pr, [-1])], [ori_h, ori_w, -1])\n",
    "\n",
    "    image = Image.fromarray(seg_img)\n",
    "    image = Image.blend(old_img, image, 0.7)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "An image for predicition is loaded. The predicition is visualized, red pixels are predicted as cracks, green pixels as spalling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = 'Test_images/2022_2198.jpg'\n",
    "\n",
    "image = detect_image(test_image_path)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the predicted images\n",
    "def detect_image_1(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = cvtColor(image)\n",
    "\n",
    "    old_img = copy.deepcopy(image)\n",
    "    ori_h = np.array(image).shape[0]\n",
    "    ori_w = np.array(image).shape[1]\n",
    "\n",
    "    image_data, nw, nh = resize_image(image, (INPUT_SHAPE[1], INPUT_SHAPE[0]))\n",
    "\n",
    "    image_data = normalize(np.array(image_data, np.float32))\n",
    "\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "\n",
    "    pr = model.predict(image_data)[0]\n",
    "\n",
    "    pr = pr[int((INPUT_SHAPE[0] - nh) // 2) : int((INPUT_SHAPE[0] - nh) // 2 + nh), \\\n",
    "            int((INPUT_SHAPE[1] - nw) // 2) : int((INPUT_SHAPE[1] - nw) // 2 + nw)]\n",
    "\n",
    "    pr = cv2.resize(pr, (ori_w, ori_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    pr = pr.argmax(axis=-1)\n",
    "\n",
    "\n",
    "    seg_img = np.reshape(\n",
    "        np.array(colors, np.uint8)[np.reshape(pr, [-1])], [ori_h, ori_w, -1])\n",
    "\n",
    "    image = Image.fromarray(seg_img)\n",
    "    #image = Image.blend(old_img, image, 0.7)\n",
    "\n",
    "    return seg_img\n",
    "\n",
    "image_1 = detect_image_1(test_image_path)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(image_1)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# save a example predicted image for convexhull\n",
    "ohg = image_1[:, :, 0]\n",
    "cv2.imwrite(\"example.png\", ohg)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4f13723ae706f6cf38c8e909b310927f2dc79f543da5c853a1f419c8fb5116f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
